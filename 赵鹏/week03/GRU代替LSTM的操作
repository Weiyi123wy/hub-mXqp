import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import warnings

warnings.filterwarnings('ignore')

# ===================== 1. 数据加载与预处理 =====================
# 读取数据集（添加异常处理）
try:
    dataset = pd.read_csv("/dataset.csv", sep="\t", header=None)
except FileNotFoundError:
    print("警告：未找到dataset.csv文件，使用模拟数据演示")
    # 生成模拟数据（方便测试）
    import random

    texts = [f"test_text_{i}_{random.randint(0, 10)}" for i in range(1000)]
    string_labels = [f"label_{random.randint(0, 2)}" for _ in range(1000)]
else:
    texts = dataset[0].tolist()
    string_labels = dataset[1].tolist()

# 标签映射（字符串标签转数字）
label_to_index = {label: i for i, label in enumerate(set(string_labels))}
numerical_labels = [label_to_index[label] for label in string_labels]

# 字符词汇表构建（字符转数字，添加<pad>填充符）
char_to_index = {'<pad>': 0}
for text in texts:
    # 处理空文本
    if pd.isna(text):
        continue
    for char in str(text):  # 确保是字符串类型
        if char not in char_to_index:
            char_to_index[char] = len(char_to_index)

index_to_char = {i: char for char, i in char_to_index.items()}
vocab_size = len(char_to_index)
max_len = 40  # 文本最大长度


# ===================== 2. 自定义数据集类 =====================
class CharGRUDataset(Dataset):  # 仅修改类名，逻辑不变
    def __init__(self, texts, labels, char_to_index, max_len):
        self.texts = texts
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.char_to_index = char_to_index
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        # 处理空文本/非字符串文本
        if pd.isna(text):
            indices = [0] * self.max_len
        else:
            text = str(text)
            # 截断+填充到固定长度
            indices = [self.char_to_index.get(char, 0) for char in text[:self.max_len]]
            indices += [0] * (self.max_len - len(indices))
        return torch.tensor(indices, dtype=torch.long), self.labels[idx]


# ===================== 3. GRU模型定义（核心修改） =====================
class GRUClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.1):
        super(GRUClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # 指定padding_idx优化训练
        # 核心修改：将LSTM替换为GRU
        self.gru = nn.GRU(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0  # 多层GRU才启用dropout
        )
        self.dropout = nn.Dropout(dropout)  # 添加dropout防止过拟合
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # x: [batch_size, max_len]
        embedded = self.embedding(x)  # [batch_size, max_len, embedding_dim]

        # 核心修改：GRU只有隐藏状态（hidden_state），没有细胞状态（cell_state）
        gru_out, hidden_state = self.gru(embedded)  # gru_out: [batch_size, max_len, hidden_dim]

        # 取最后一层的最后一个时间步的隐藏状态
        hidden = self.dropout(hidden_state[-1])  # [batch_size, hidden_dim]
        out = self.fc(hidden)  # [batch_size, output_dim]
        return out


# ===================== 4. 训练配置 =====================
# 设备配置（自动检测GPU/CPU）
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用设备: {device}")

# 构建数据集和数据加载器（仅修改数据集类名）
gru_dataset = CharGRUDataset(texts, numerical_labels, char_to_index, max_len)
dataloader = DataLoader(gru_dataset, batch_size=32, shuffle=True, num_workers=0)  # num_workers=0避免Windows系统报错

# 模型参数（与原LSTM一致）
embedding_dim = 64
hidden_dim = 128
output_dim = len(label_to_index)
num_epochs = 4

# 初始化GRU模型并移到指定设备（核心修改：使用GRUClassifier）
model = GRUClassifier(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)
criterion = nn.CrossEntropyLoss()  # 分类任务损失函数
optimizer = optim.Adam(model.parameters(), lr=0.001)  # 优化器

# ===================== 5. 模型训练（完全不变） =====================
print("开始训练GRU模型...")
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for idx, (inputs, labels) in enumerate(dataloader):
        # 数据移到指定设备
        inputs, labels = inputs.to(device), labels.to(device)

        # 梯度清零
        optimizer.zero_grad()
        # 前向传播
        outputs = model(inputs)
        # 计算损失
        loss = criterion(outputs, labels)
        # 反向传播
        loss.backward()
        # 更新参数
        optimizer.step()

        running_loss += loss.item()
        if idx % 10 == 0:  # 每10个batch打印一次（模拟数据batch少，调整频率）
            print(f"Epoch [{epoch + 1}/{num_epochs}], Batch [{idx}], Loss: {loss.item():.4f}")

    avg_loss = running_loss / len(dataloader)
    print(f"Epoch [{epoch + 1}/{num_epochs}] 完成, 平均Loss: {avg_loss:.4f}")


# ===================== 6. 预测函数（完全不变） =====================
def classify_text_gru(text, model, char_to_index, max_len, label_to_index, device):
    """
    对单条文本进行分类预测（GRU版本）
    :param text: 输入文本
    :param model: 训练好的GRU模型
    :param char_to_index: 字符到索引的映射
    :param max_len: 文本最大长度
    :param label_to_index: 标签到索引的映射
    :param device: 运行设备
    :return: 预测的标签字符串
    """
    # 构建反向标签映射
    index_to_label = {i: label for label, i in label_to_index.items()}

    # 文本预处理
    if pd.isna(text):
        indices = [0] * max_len
    else:
        text = str(text)
        indices = [char_to_index.get(char, 0) for char in text[:max_len]]
        indices += [0] * (max_len - len(indices))

    # 转换为tensor并添加batch维度
    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)

    # 模型推理（关闭梯度计算）
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)
        _, predicted_idx = torch.max(output, 1)  # 取概率最大的类别
        predicted_label = index_to_label[predicted_idx.item()]

    return predicted_label


# ===================== 7. 测试预测 =====================
# 测试示例
test_text = "这是一条测试文本"  # 替换成你要测试的文本
predicted_label = classify_text_gru(
    test_text, model, char_to_index, max_len, label_to_index, device
)
print(f"\n测试文本: {test_text}")
print(f"预测标签: {predicted_label}")
