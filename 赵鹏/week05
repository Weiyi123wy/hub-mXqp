作业1
阅读上面的客服工作台的说明，总结我们自己作为后端开发和算法开发的角色，我们需要做什么？
    ◦ 需要设计数据库吗？
    ◦ 需要使用什么模型？
    ◦ 如何使用使用bert的？
    ◦ 是否需要使用大模型？

客服工作台后端与算法开发工作总结

作为后端开发与算法开发，针对该客服工作台 FAQ 管理与智能匹配应答的核心需求，需从数据层设计、算法模型选型与落地、功能模块开发三方面落地开发工作，保障用户提问与历史问题的智能相似度匹配及 FAQ 全生命周期管理，以下为核心工作内容：

后端开发核心工作：需设计适配 FAQ 类目管理与信息存储的数据库，拆解核心实体为类目表（含一级 / 二级类目 ID、名称、层级关系、父类 ID）、FAQ 信息表（含 FAQID、标题、标准问法、答案内容、答案类型、生效时间、标签、所属类目 ID）、相似问法表（含关联 FAQID、相似问题内容）、环境配置表（区分测试 / 正式环境的 FAQ 状态、发布记录），同时实现类目增删改查、FAQ 录入 / 编辑 / 批量操作 / 环境同步、数据检索与筛选的接口开发，保障前端工作台的各类操作与数据流转。

算法开发核心工作：核心采用 BERT 模型完成用户提问与历史问题的相似度匹配，将 BERT 作为文本编码基础模型，对标准问法、相似问法及用户实时提问进行统一编码，转化为高维语义向量，通过计算向量间的余弦相似度，匹配出最相似的历史提问并返回对应答案；暂无需直接使用大模型，当前场景以固定 FAQ 的语义匹配为核心，轻量的 BERT 模型即可满足精准度与响应速度要求。

协同开发与功能落地：算法层需为后端提供相似度计算的调用接口，后端将前端传入的用户提问参数传递至算法模块，接收匹配结果后关联数据库中的答案信息返回；同时需开发相似问法的语义校验逻辑，避免重复录入，保障 BERT 模型的匹配效率，兼顾 FAQ 生效时间的过滤逻辑，确保仅返回有效期内的问答结果。

作业2

BERT 文本编码与相似度计算技术方案
一、核心技术方案
基于 BERT 模型实现文本编码与相似度计算，核心为 “文本预处理→BERT 语义编码→向量相似度计算” 三步流程，适配客服工作台用户提问与 FAQ 历史问题的匹配场景，具体方案如下：

文本预处理：对用户提问、FAQ 标准问法 / 相似问法统一处理，包括中文分词（Jieba 分词）、去停用词（过滤 “的 / 了 / 吗” 等无意义词汇）、文本长度截断（BERT 输入长度限制为 512 个字符），确保输入文本符合模型要求；

BERT 语义编码：采用中文轻量版 BERT 模型（如 bert-base-chinese），将预处理后的文本转化为模型可识别的 Token（字符编码），输入 BERT 模型的 Transformer 编码器，提取 [CLS] 位置的输出向量（768 维）作为文本的语义向量，实现自然语言到高维语义空间的映射；

相似度计算：通过余弦相似度算法计算用户提问向量与 FAQ 问题向量的相似度值（取值范围 0-1），公式为（相似度 = 用户提问向量与 FAQ 问题向量的点积  / 二者的模长开根号），其中 A、B 为两个文本的语义向量，值越接近 1 表示语义越相似，取相似度最高的 FAQ 对应的答案返回给用户。
